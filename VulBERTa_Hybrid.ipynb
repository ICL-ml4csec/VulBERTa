{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "import torchtext.vocab as vocab\n",
    "import sklearn.metrics\n",
    "from transformers import RobertaModel\n",
    "from transformers import RobertaConfig\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "from torch.optim import SGD,Adam,RMSprop\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "from clang import *\n",
    "\n",
    "\n",
    "seed = 1234\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cindex.Config.set_library_file('/usr/lib/llvm-10/lib/libclang-10.so.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "multigpu = False\n",
    "if device == torch.device('cuda'):\n",
    "\tmultigpu = torch.cuda.device_count() > 1\n",
    "print('Device: ',device)\n",
    "print('MultiGPU: ',multigpu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training & vocab parameters\n",
    "DATA_PATH = 'data'\n",
    "VOCAB_SIZE = 50000\n",
    "BATCH_SIZE = 128\n",
    "EMBED_SIZE = VOCAB_SIZE+2\n",
    "EMBED_DIM = 768 #768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#### LATEST Vocab 1\n",
    "\n",
    "from tokenizers.pre_tokenizers import PreTokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers import NormalizedString,PreTokenizedString\n",
    "from typing import List\n",
    "\n",
    "class MyTokenizer:\n",
    "    \n",
    "    cindex.Config.set_library_file('/usr/lib/llvm-10/lib/libclang-10.so.1')\n",
    "    cidx = cindex.Index.create()\n",
    "    \n",
    "\n",
    "    def clang_split(self, i: int, normalized_string: NormalizedString) -> List[NormalizedString]:\n",
    "        ## Tokkenize using clang\n",
    "        tok = []\n",
    "        tu = self.cidx.parse('tmp.cpp',\n",
    "                       args=[''],  \n",
    "                       unsaved_files=[('tmp.cpp', str(normalized_string))],  \n",
    "                       options=0)\n",
    "        for t in tu.get_tokens(extent=tu.cursor.extent):\n",
    "            spelling = t.spelling#.strip()\n",
    "            tkind = str(t.kind)\n",
    "            ckind = str(t.cursor.kind)\n",
    "            \n",
    "#             if spelling == '':\n",
    "#                 continue\n",
    "                \n",
    "#             myspelling = t.spelling.replace(' ', '')\n",
    "\n",
    "            ## Keyword no need\n",
    "\n",
    "            ## Punctuations no need\n",
    "\n",
    "            ## Literal\n",
    "            if tkind == \"TokenKind.LITERAL\":\n",
    "                if ckind == \"CursorKind.INTEGER_LITERAL\":\n",
    "                    if 'x' in spelling:\n",
    "                        tok.append(NormalizedString('LITERAL_INT_HEX'))\n",
    "                    elif 'ul' in spelling.lower():\n",
    "                        tok.append(NormalizedString('LITERAL_INT_UL'))\n",
    "                    elif 'u' in spelling.lower():\n",
    "                        tok.append(NormalizedString('LITERAL_INT_U'))\n",
    "                    elif 'l' in spelling.lower():\n",
    "                        tok.append(NormalizedString('LITERAL_INT_L'))\n",
    "                    else:\n",
    "                        tok.append(NormalizedString('LITERAL_INT_INT'))\n",
    "\n",
    "                elif ckind == \"CursorKind.FLOATING_LITERAL\":\n",
    "                    if 'e' in spelling:\n",
    "                        tok.append(NormalizedString('LITERAL_FLOAT_EXP'))\n",
    "                    elif 'l' in spelling.lower():\n",
    "                        tok.append(NormalizedString('LITERAL_FLOAT_LF'))\n",
    "                    else:\n",
    "                        tok.append(NormalizedString('LITERAL_FLOAT_FLOAT'))\n",
    "\n",
    "                elif ckind == \"CursorKind.STRING_LITERAL\":\n",
    "                    if spelling == \"\\\"\\\"\":\n",
    "                        tok.append(NormalizedString('LITERAL_STRING_EMPTY'))\n",
    "                    elif spelling[1:-1].isnumeric() == True:\n",
    "                        tok.append(NormalizedString('LITERAL_STRING_NUMERIC'))\n",
    "                    elif re.match(regex_def.FILE_EXTENSION, spelling) is not None:\n",
    "                        tok.append(NormalizedString('LITERAL_STRING_FILEEXTENSION'))\n",
    "                    elif re.match(regex_def.BOOLEAN, spelling) is not None:\n",
    "                        tok.append(NormalizedString('LITERAL_STRING_BOOLEAN'))\n",
    "                    elif re.match(regex_def.LOGICAL_OP, spelling) is not None:\n",
    "                        tok.append(NormalizedString('LITERAL_STRING_LOGICALOP'))\n",
    "                    elif re.match(regex_def.BITWISE_OP, spelling) is not None:\n",
    "                        tok.append(NormalizedString('LITERAL_STRING_BITWISEOP'))\n",
    "                    elif re.match(regex_def.COMPARISON_OP, spelling) is not None:\n",
    "                        tok.append(NormalizedString('LITERAL_STRING_COMPARISONOP'))\n",
    "                    elif re.match(regex_def.ARITHMETIC_OP, spelling) is not None:\n",
    "                        tok.append(NormalizedString('LITERAL_STRING_ARITHMETICOP'))\n",
    "                    elif re.match(regex_def.CONTROLFLOW, spelling) is not None:\n",
    "                        tok.append(NormalizedString('LITERAL_STRING_CONTROLFLOW'))\n",
    "                    elif re.match(regex_def.IPV4, spelling) is not None:\n",
    "                        tok.append(NormalizedString('LITERAL_STRING_IPV4'))\n",
    "                    elif re.match(regex_def.IPV6, spelling) is not None:\n",
    "                        tok.append(NormalizedString('LITERAL_STRING_IPV6'))\n",
    "                    elif re.match(regex_def.MIMETYPE, spelling) is not None:\n",
    "                        tok.append(NormalizedString('LITERAL_STRING_MIMETYPE'))\n",
    "                    elif re.match(regex_def.FILEPATH, spelling) is not None:\n",
    "                        tok.append(NormalizedString('LITERAL_STRING_FILEPATH'))\n",
    "                    elif re.match(regex_def.LANGUAGE_ISO, spelling) is not None:\n",
    "                        tok.append(NormalizedString('LITERAL_STRING_LANGUAGEISO'))\n",
    "                    elif re.match(regex_def.STATUS, spelling) is not None:\n",
    "                        tok.append(NormalizedString('LITERAL_STRING_STATUS'))\n",
    "                    elif re.match(regex_def.MONTH, spelling) is not None:\n",
    "                        tok.append(NormalizedString('LITERAL_STRING_MONTH'))\n",
    "                    elif re.match(regex_def.SQL_STATEMENT, spelling) is not None:\n",
    "                        tok.append(NormalizedString('LITERAL_STRING_SQLSTATEMENT'))\n",
    "                    elif re.match(regex_def.TIME, spelling) is not None:\n",
    "                        tok.append(NormalizedString('LITERAL_STRING_TIME'))\n",
    "                    elif re.match(regex_def.NEWLINE, spelling) is not None:\n",
    "                        tok.append(NormalizedString('LITERAL_STRING_NEWLINE'))\n",
    "                    elif re.match(regex_def.BASH_BASIC, spelling) is not None:\n",
    "                        tok.append(NormalizedString('LITERAL_STRING_BASH'))\n",
    "                    else:\n",
    "                        tok.append(NormalizedString('LITERAL_STRING_UNK'))\n",
    "\n",
    "                elif ckind == \"CursorKind.CHARACTER_LITERAL\":\n",
    "                    if 'x' in spelling[1:-1]:\n",
    "                        tok.append(NormalizedString('LITERAL_CHAR_HEX'))\n",
    "                    elif spelling[1:-1].isalpha() == True:\n",
    "                        tok.append(NormalizedString('LITERAL_CHAR_CHAR'))\n",
    "                    elif spelling[1:-1].isdigit() == True:\n",
    "                        tok.append(NormalizedString('LITERAL_CHAR_INT'))\n",
    "                    else:\n",
    "                        tok.append(NormalizedString('LITERAL_CHAR_UNK'))\n",
    "\n",
    "                elif ckind == \"CursorKind.ASM_STMT\":\n",
    "                    tok.append(NormalizedString('LITERAL_ASM'))\n",
    "\n",
    "                elif ckind == \"CursorKind.LABEL_STMT\":\n",
    "                    tok.append(NormalizedString('LITERAL_LABELSTMT'))\n",
    "\n",
    "                elif ckind == \"CursorKind.VAR_DECL\":\n",
    "                    if '0x' in spelling:\n",
    "                        tok.append(NormalizedString('LITERAL_VARDECL_HEX'))\n",
    "                    elif spelling.isnumeric() == True:\n",
    "                        tok.append(NormalizedString('LITERAL_VARDECL_NUM'))\n",
    "                    else:\n",
    "                        tok.append(NormalizedString('LITERAL_VARDECL_STRING'))\n",
    "\n",
    "                elif ckind == \"CursorKind.DECL_STMT\":\n",
    "                    if '0x' in spelling:\n",
    "                        tok.append(NormalizedString('LITERAL_DECLSTMT_HEX'))\n",
    "                    elif spelling.isnumeric() == True:\n",
    "                        tok.append(NormalizedString('LITERAL_DECLSTMT_NUM'))\n",
    "                    elif spelling.replace('.','').isnumeric() == True:\n",
    "                        tok.append(NormalizedString('LITERAL_DECLSTMT_FLOAT'))\n",
    "                    else:\n",
    "                        tok.append(NormalizedString('LITERAL_DECLSTMT_STRING'))\n",
    "\n",
    "                else:\n",
    "                    tok.append(NormalizedString('LITERAL_UNK'))\n",
    "\n",
    "            elif tkind == \"TokenKind.IDENTIFIER\":\n",
    "                if ckind == \"CursorKind.MEMBER_REF_EXPR\":\n",
    "                    tok.append(NormalizedString('IDENTIFIER_MEMBERREFEXPR'))\n",
    "\n",
    "                elif ckind == \"CursorKind.UNARY_OPERATOR\":\n",
    "                    tok.append(NormalizedString('IDENTIFIER_UNARYOP'))\n",
    "\n",
    "                elif ckind == \"CursorKind.CONSTRUCTOR\":\n",
    "                    tok.append(NormalizedString('IDENTIFIER_CONSTRUCTOR'))\n",
    "\n",
    "                elif ckind == \"CursorKind.CLASS_DECL\":\n",
    "                    tok.append(NormalizedString('IDENTIFIER_CLASSDECL'))\n",
    "\n",
    "                elif ckind == \"CursorKind.DEFAULT_STMT\":\n",
    "                    tok.append(NormalizedString('IDENTIFIER_DEFAULTSTMT'))\n",
    "\n",
    "                elif ckind == \"CursorKind.CSTYLE_CAST_EXPR\":\n",
    "                    tok.append(NormalizedString('IDENTIFIER_CSTYLECASTEXPR'))\n",
    "\n",
    "                elif ckind == \"CursorKind.COMPOUND_ASSIGNMENT_OPERATOR\":\n",
    "                    tok.append(NormalizedString('IDENTIFIER_COMPOUNDASSIGNMENTOPERATOR'))\n",
    "\n",
    "                elif ckind == \"CursorKind.WHILE_STMT\":\n",
    "                    tok.append(NormalizedString('IDENTIFIER_WHILESTMT'))\n",
    "\n",
    "                elif ckind == \"CursorKind.DO_STMT\":\n",
    "                    tok.append(NormalizedString('IDENTIFIER_DOSTMT'))\n",
    "\n",
    "                elif ckind == \"CursorKind.DESTRUCTOR\":\n",
    "                    tok.append(NormalizedString('IDENTIFIER_DESTRUCTOR'))\n",
    "\n",
    "                elif ckind == \"CursorKind.MEMBER_REF\":\n",
    "                    tok.append(NormalizedString('IDENTIFIER_MEMBERREF'))\n",
    "\n",
    "                elif ckind == \"CursorKind.ADDR_LABEL_EXPR\":\n",
    "                    tok.append(NormalizedString('IDENTIFIER_ADDRLABELEXPR'))\n",
    "\n",
    "                elif ckind == \"CursorKind.PACKED_ATTR\":\n",
    "                    tok.append(NormalizedString('IDENTIFIER_PACKEDATTR'))\n",
    "\n",
    "                elif ckind == \"CursorKind.ASM_STMT\":\n",
    "                    tok.append(NormalizedString('IDENTIFIER_ASMSTMT'))\n",
    "\n",
    "                elif ckind == \"CursorKind.CALL_EXPR\":\n",
    "                    tok.append(NormalizedString('IDENTIFIER_CALLEXPR'))\n",
    "\n",
    "                elif ckind == \"CursorKind.UNEXPOSED_DECL\":\n",
    "                    tok.append(NormalizedString('IDENTIFIER_UNEXPOSEDDECL'))\n",
    "\n",
    "                elif ckind == \"CursorKind.CXX_NEW_EXPR\":\n",
    "                    tok.append(NormalizedString('IDENTIFIER_CXXNEWEXPR'))\n",
    "\n",
    "                elif ckind == \"CursorKind.CXX_METHOD\":\n",
    "                    tok.append(NormalizedString('IDENTIFIER_CXXMETHOD'))\n",
    "\n",
    "                elif ckind == \"CursorKind.CXX_DELETE_EXPR\":\n",
    "                    tok.append(NormalizedString('IDENTIFIER_CXXDELETEEXPR'))\n",
    "\n",
    "                elif ckind == \"CursorKind.ALIGNED_ATTR\":\n",
    "                    tok.append(NormalizedString('IDENTIFIER_ALIGNEDATTR'))\n",
    "\n",
    "                elif ckind == \"CursorKind.STRING_LITERAL\":\n",
    "                    tok.append(NormalizedString('IDENTIFIER_STRINGLITERAL'))\n",
    "\n",
    "                elif ckind == \"CursorKind.LAMBDA_EXPR\":\n",
    "                    tok.append(NormalizedString('IDENTIFIER_LAMBDAEXPR'))\n",
    "\n",
    "                elif ckind == \"CursorKind.CXX_TRY_STMT\":\n",
    "                    tok.append(NormalizedString('IDENTIFIER_CXXTRYSTMT'))\n",
    "\n",
    "                elif ckind == \"CursorKind.CONDITIONAL_OPERATOR\":\n",
    "                    tok.append(NormalizedString('IDENTIFIER_CONDITIONALOP'))\n",
    "\n",
    "                elif ckind == \"CursorKind.CXX_REINTERPRET_CAST_EXPR\":\n",
    "                    tok.append(NormalizedString('IDENTIFIER_CXXREINTERPRETCASTEXPR'))\n",
    "\n",
    "                elif ckind == \"CursorKind.SWITCH_STMT\":\n",
    "                    tok.append(NormalizedString('IDENTIFIER_SWITCHSTMT'))\n",
    "\n",
    "                elif ckind == \"CursorKind.UNEXPOSED_ATTR\":\n",
    "                    tok.append(NormalizedString('IDENTIFIER_UNEXPOSEDATTR'))\n",
    "\n",
    "                elif ckind == \"CursorKind.NAMESPACE_REF\":\n",
    "                    tok.append(NormalizedString('IDENTIFIER_NAMESPACEREF'))\n",
    "\n",
    "                elif ckind == \"CursorKind.RETURN_STMT\":\n",
    "                    tok.append(NormalizedString('IDENTIFIER_RETURNSTMT'))\n",
    "\n",
    "                elif ckind == \"CursorKind.PAREN_EXPR\":\n",
    "                    tok.append(NormalizedString('IDENTIFIER_PARENEXPR'))\n",
    "\n",
    "                else:\n",
    "                    tok.append(NormalizedString(spelling))\n",
    "\n",
    "            else:\n",
    "                tok.append(NormalizedString(spelling))\n",
    "\n",
    "        return(tok)\n",
    "    \n",
    "    def pre_tokenize(self, pretok: PreTokenizedString):\n",
    "        pretok.split(self.clang_split)\n",
    "        \n",
    "        \n",
    "        \n",
    "## Custom tokenizer\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import StripAccents\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers import processors,pre_tokenizers\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "## Init\n",
    "#my_tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "#my_tokenizer = Tokenizer(BPE())\n",
    "\n",
    "\n",
    "## Load\n",
    "vocab, merges = BPE.read_file(vocab=\"./tokenizer5/v1/draper-vocab.json\", merges=\"./tokenizer5/v1/draper-merges.txt\")\n",
    "my_tokenizer = Tokenizer(BPE(vocab, merges, unk_token=\"<unk>\"))\n",
    "\n",
    "\n",
    "mf = [(\"<s>\",0),(\"<pad>\",1),(\"</s>\",2),(\"<unk>\",3),(\"<mask>\",4)]\n",
    "mf = mf + [('LITERAL_INT_INT',5),('LITERAL_INT_UL',6),('LITERAL_INT_U',7),('LITERAL_INT_L',8),('LITERAL_INT_HEX',9)]\n",
    "mf = mf + [('LITERAL_FLOAT_FLOAT',10),('LITERAL_FLOAT_EXP',11),('LITERAL_FLOAT_LF',12)]\n",
    "mf = mf + [('LITERAL_STRING_EMPTY',13),('LITERAL_STRING_NUMERIC',14),('LITERAL_STRING_FILEEXTENSION',15),('LITERAL_STRING_BOOLEAN',16),('LITERAL_STRING_LOGICALOP',17),('LITERAL_STRING_BITWISEOP',18),('LITERAL_STRING_COMPARISONOP',19),('LITERAL_STRING_ARITHMETICOP',20),('LITERAL_STRING_CONTROLFLOW',21),('LITERAL_STRING_IPV4',22),('LITERAL_STRING_IPV6',23),('LITERAL_STRING_MIMETYPE',24),('LITERAL_STRING_FILEPATH',25),('LITERAL_STRING_LANGUAGEISO',26),('LITERAL_STRING_STATUS',27),('LITERAL_STRING_MONTH',28),('LITERAL_STRING_SQLSTATEMENT',29),('LITERAL_STRING_TIME',30),('LITERAL_STRING_NEWLINE',31),('LITERAL_STRING_BASH',32),('LITERAL_STRING_UNK',33)]\n",
    "mf = mf + [('LITERAL_CHAR_HEX',34),('LITERAL_CHAR_CHAR',35),('LITERAL_CHAR_INT',36),('LITERAL_CHAR_UNK',37)]\n",
    "mf = mf + [('LITERAL_ASM',38),('LITERAL_LABELSTMT',39)]\n",
    "mf = mf + [('LITERAL_VARDECL_HEX',40), ('LITERAL_VARDECL_NUM',41),('LITERAL_VARDECL_STRING',42)]\n",
    "mf = mf + [('LITERAL_DECLSTMT_HEX',43),('LITERAL_DECLSTMT_NUM',44),('LITERAL_DECLSTMT_FLOAT',45),('LITERAL_DECLSTMT_STRING',46)]\n",
    "mf = mf + [('LITERAL_UNK',47)]\n",
    "mf = mf + [('IDENTIFIER_MEMBERREFEXPR',48),('IDENTIFIER_UNARYOP',49),('IDENTIFIER_CONSTRUCTOR',50),('IDENTIFIER_CLASSDECL',51),('IDENTIFIER_DEFAULTSTMT',52),('IDENTIFIER_CSTYLECASTEXPR',53),('IDENTIFIER_COMPOUNDASSIGNMENTOPERATOR',54),('IDENTIFIER_WHILESTMT',55),('IDENTIFIER_DOSTMT',56),('IDENTIFIER_DESTRUCTOR',57),('IDENTIFIER_MEMBERREF',58),('IDENTIFIER_ADDRLABELEXPR',59),('IDENTIFIER_PACKEDATTR',60),('IDENTIFIER_ASMSTMT',61),('IDENTIFIER_CALLEXPR',62),('IDENTIFIER_UNEXPOSEDDECL',63),('IDENTIFIER_CXXNEWEXPR',64),('IDENTIFIER_CXXMETHOD',65),('IDENTIFIER_CXXDELETEEXPR',66),('IDENTIFIER_ALIGNEDATTR',67),('IDENTIFIER_STRINGLITERAL',68),('IDENTIFIER_LAMBDAEXPR',69),('IDENTIFIER_CXXTRYSTMT',70),('IDENTIFIER_CONDITIONALOP',71),('IDENTIFIER_CXXREINTERPRETCASTEXPR',72),('IDENTIFIER_SWITCHSTMT',73),('IDENTIFIER_UNEXPOSEDATTR',74),('IDENTIFIER_NAMESPACEREF',75),('IDENTIFIER_RETURNSTMT',76),('IDENTIFIER_PARENEXP',77)]\n",
    "mf = mf + [('char',78),('int',79),('switch',80),('case',81),('if',82),('break',83),('for',84),('const',85),('unsigned',86),('struct',87),('default',88),('return',89),('long',90),('goto',91),('this',92),('enum',93),('bool',94),('static',95),('false',96),('true',97),('new',98),('delete',99),('while',100),('double',101),('else',102),('private',103),('do',104),('sizeof',105),('void',106),('continue',107),('__attribute__',108),('short',109),('throw',110),('float',111),('register',112),('__FUNCTION__',113),('static_cast',114),('__func__',115),('class',116),('try',117),('dynamic_cast',118),('template',119),('union',120),('reinterpret_cast',121),('catch',122),('operator',123),('const_cast',124),('using',125),('namespace',126),('typename',127),('wchar_t',128),('not',129),('typeof',130),('__label__',131),('__PRETTY_FUNCTION__',132),('auto',133),('__extension__',134),('volatile',135),('__asm__',136),('__volatile__',137),('extern',138),('asm',139),('signed',140),('typedef',141),('typeid',142),('and',143),('or',144),('public',145),('virtual',146),('nullptr',147),('__restrict',148),('__asm',149),('__typeof__',150),('xor',151),('__complex__',152),('__real__',153),('__imag__',154),('not_eq',155),('export',156),('compl',157),('__alignof__',158),('__restrict__',159),('__cdecl',160),('bitor',161),('protected',162),('explicit',163),('friend',164),('decltype',165),('mutable',166),('inline',167),('__const',168),('__stdcall',169),('char16_t',170),('char32_t',171),('_Decimal64',172),('constexpr',173),('bitand',174),('alignof',175),('static_assert',176),('__attribute',177),('thread_local',178),('__alignof',179),('__builtin_va_arg',180),('_Decimal32',181)]\n",
    "mf = mf + [('\"',182),('(',183),('*',184),(',',185),(')',186),('{',187),(';',188),('->',189),(':',190),('.',191),('-',192),('=',193),('+',194),('<',195),('++',196),('+=',197),('==',198),('||',199),('!=',200),('}',201),('/',202),('!',203),('>=',204),('[',205),(']',206),('&',207),('::',208),('&&',209),('>',210),('#',211),('--',212),('<=',213),('-=',214),('|',215),('%',216),('?',217),('<<',218),('>>',219),('|=',220),('&=',221),('^',222),('~',223),('^=',224),('...',225),('/=',226),('*=',227),('>>=',228),('<<=',229),('%=',230),('##',231),('->*',232),('\\\\',233),('.*',234),('@',235)]\n",
    "\n",
    "my_tokenizer.normalizer = normalizers.Sequence([StripAccents()])\n",
    "my_tokenizer.pre_tokenizer = PreTokenizer.custom(MyTokenizer())\n",
    "my_tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "my_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"<s> $A </s>\",\n",
    "    special_tokens=[\n",
    "    (\"<s>\",0),\n",
    "    (\"<pad>\",1),\n",
    "    (\"</s>\",2),\n",
    "    (\"<unk>\",3),\n",
    "    (\"<mask>\",4),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#### LATEST Vocab 2\n",
    "\n",
    "from tokenizers.pre_tokenizers import PreTokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers import NormalizedString,PreTokenizedString\n",
    "from typing import List\n",
    "\n",
    "class MyTokenizer:\n",
    "    \n",
    "    cindex.Config.set_library_file('/usr/lib/llvm-10/lib/libclang-10.so.1')\n",
    "    cidx = cindex.Index.create()\n",
    "    \n",
    "\n",
    "    def clang_split(self, i: int, normalized_string: NormalizedString) -> List[NormalizedString]:\n",
    "        ## Tokkenize using clang\n",
    "        tok = []\n",
    "        tu = self.cidx.parse('tmp.cpp',\n",
    "                       args=[''],  \n",
    "                       unsaved_files=[('tmp.cpp', str(normalized_string))],  \n",
    "                       options=0)\n",
    "        for t in tu.get_tokens(extent=tu.cursor.extent):\n",
    "            spelling = t.spelling.strip()\n",
    "            tkind = str(t.kind)\n",
    "            ckind = str(t.cursor.kind)\n",
    "            \n",
    "            if spelling == '':\n",
    "                continue\n",
    "                \n",
    "            spelling = spelling.replace(' ', '')\n",
    "\n",
    "            tok.append(NormalizedString(spelling))\n",
    "\n",
    "        return(tok)\n",
    "    \n",
    "    def pre_tokenize(self, pretok: PreTokenizedString):\n",
    "        pretok.split(self.clang_split)\n",
    "        \n",
    "## Custom tokenizer\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers import normalizers,decoders\n",
    "from tokenizers.normalizers import StripAccents, unicode_normalizer_from_str\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers import processors,pre_tokenizers\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "## Init\n",
    "#my_tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "#my_tokenizer = Tokenizer(BPE())\n",
    "\n",
    "\n",
    "## Load\n",
    "vocab, merges = BPE.read_file(vocab=\"./tokenizer5/v2/draper-vocab.json\", merges=\"./tokenizer5/v2/draper-merges.txt\")\n",
    "my_tokenizer = Tokenizer(BPE(vocab, merges, unk_token=\"<unk>\"))\n",
    "\n",
    "mf = [(\"<s>\",0),(\"<pad>\",1),(\"</s>\",2),(\"<unk>\",3),(\"<mask>\",4)]\n",
    "mf = mf + [('char',5),('int',6),('switch',7),('case',8),('if',9),('break',10),('for',11),('const',12),('unsigned',13),('struct',14),('default',15),('return',16),('long',17),('goto',18),('this',19),('enum',20),('bool',21),('static',22),('false',23),('true',24),('new',25),('delete',26),('while',27),('double',28),('else',29),('private',30),('do',31),('sizeof',32),('void',33),('continue',34),('__attribute__',35),('short',36),('throw',37),('float',38),('register',39),('__FUNCTION__',40),('static_cast',41),('__func__',42),('class',43),('try',44),('dynamic_cast',45),('template',46),('union',47),('reinterpret_cast',48),('catch',49),('operator',50),('const_cast',51),('using',52),('namespace',53),('typename',54),('wchar_t',55),('not',56),('typeof',57),('__label__',58),('__PRETTY_FUNCTION__',59),('auto',60),('__extension__',61),('volatile',62),('__asm__',63),('__volatile__',64),('extern',65),('asm',66),('signed',67),('typedef',68),('typeid',69),('and',70),('or',71),('public',72),('virtual',73),('nullptr',74),('__restrict',75),('__asm',76),('__typeof__',77),('xor',78),('__complex__',79),('__real__',80),('__imag__',81),('not_eq',82),('export',83),('compl',84),('__alignof__',85),('__restrict__',86),('__cdecl',87),('bitor',88),('protected',89),('explicit',90),('friend',91),('decltype',92),('mutable',93),('inline',94),('__const',95),('__stdcall',96),('char16_t',97),('char32_t',98),('_Decimal64',99),('constexpr',100),('bitand',101),('alignof',102),('static_assert',103),('__attribute',104),('thread_local',105),('__alignof',106),('__builtin_va_arg',107),('_Decimal32',108)]\n",
    "mf = mf + [('\\\"',109),('(',110),('*',111),(',',112),(')',113),('{',114),(';',115),('->',116),(':',117),('.',118),('-',119),('=',120),('+',121),('<',122),('++',123),('+=',124),('==',125),('||',126),('!=',127),('}',128),('/',129),('!',130),('>=',131),('[',132),(']',133),('&',134),('::',135),('&&',136),('>',137),('#',138),('--',139),('<=',140),('-=',141),('|',142),('%',143),('?',144),('<<',145),('>>',146),('|=',147),('&=',148),('^',149),('~',150),('^=',151),('...',152),('/=',153),('*=',154),('>>=',155),('<<=',156),('%=',157),('##',158),('->*',159),('\\\\',160),('.*',161),('@',162)]\n",
    "\n",
    "my_tokenizer.normalizer = normalizers.Sequence([StripAccents()])\n",
    "my_tokenizer.pre_tokenizer = PreTokenizer.custom(MyTokenizer())\n",
    "my_tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "my_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"<s> $A </s>\",\n",
    "    special_tokens=[\n",
    "    (\"<s>\",0),\n",
    "    (\"<pad>\",1),\n",
    "    (\"</s>\",2),\n",
    "    (\"<unk>\",3),\n",
    "    (\"<mask>\",4),\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#### LATEST Vocab 5\n",
    "from tokenizers.pre_tokenizers import PreTokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers import NormalizedString,PreTokenizedString\n",
    "from typing import List\n",
    "\n",
    "class MyTokenizer:\n",
    "    \n",
    "    cindex.Config.set_library_file('/usr/lib/llvm-10/lib/libclang-10.so.1')\n",
    "    cidx = cindex.Index.create()\n",
    "    \n",
    "    std_api_calls = set(['_Exit','abs','acos','acosh','asctime','asin','asinh','assert','at_quick_exit','atan','atan2','atanh','atexit','atof','atol','bsearch','btowc','c16rtomb','c32rtomb','cbrt','ceil','cerr','cin','clearerr','clock','clog','copysign','cos','cosh','cout','ctime','difftime','div','errno','exp','exp2','expm1','fabs','fclose','fdim','feclearexcept','fegetenv','fegetexceptflag','fegetround','feholdexcept','feof','feraiseexcept','ferror','fesetenv','fesetexceptflag','fesetround','fetestexcept','feupdateenv','fflush','fgetc','fgetpos','fgets','fgetwc','fgetws','floor','fma','fmax','fmod','fopen','fprintf','fputc','fputs','fputwc','fputws','fread','free','freopen','frexp','fscanf','fseek','fsetpos','ftell','fwide','fwprintf','fwrite','fwscanf','getc','getchar','getenv','gets','getwc','getwchar','gmtime','hypot','ilogb','imaxabs','imaxdiv','isblank','iscntrl','isdigit','isgraph','islower','isprint','ispunct','isspace','isupper','iswalnum','iswalpha','iswblank','iswcntrl','iswctype','iswdigit','iswgraph','iswlower','iswprint','iswpunct','iswspace','iswupper','iswxdigit','isxdigit','labs','ldexp','ldiv','llabs','lldiv','llrint','llround','localeconv','localtime','log','log10','log1p','log2','logb','longjmp','lrint','lround','malloc','mblen','mbrlen','mbrtoc16','mbrtoc32','mbrtowc','mbsinit','mbsrtowcs','mbstowcs','mbtowc','memchr','memcmp','memcpy','memmove','memset','mktime','modf','nan','nearbyint','nextafter','nexttoward','perror','pow','printf','putc','putchar','puts','putwchar','qsort','quick_exit','raise','realloc','remainder','remove','remquo','rename','rewind','rint','round','sca','scalbln','scalbn','setbuf','setjmp','setlocale','setvbuf','signal','sin','sinh','snprintf','sprintf','sqrt','srand','sscanf','strcat','strchr','strcmp','strcoll','strcpy','strcspn','strerror','strftime','strlen','strncat','strncmp','strncpy','strpbrk','strrchr','strspn','strstr','strtod','strtoimax','strtok','strtol','strtoll','strtoull','strtoumax','strxfrm','swprintf','swscanf','tan','tanh','time','tmpfile','tmpnam','tolower','toupper','towctrans','towlower','towupper','trunc','ungetc','ungetwc','vfprintf','vfscanf','vfwprintf','vfwscanf','vprintf','vscanf','vsfscanf','vsnprintf','vsprintf','vsscanf','vswprintf','vwprintf','vwscanf','wcerr','wcin','wclog','wcout','wcrtomb','wcscat','wcschr','wcscmp','wcscoll','wcscpy','wcscspn','wcsftime','wcslne','wcsncat','wcsncmp','wcsncpy','wcspbrk','wcsrchr','wcsrtombs','wcsspn','wcsstr','wcstod','wcstof','wcstoimax','wcstok','wcstol','wcstold','wcstoll','wcstombs','wcstoul','wcstoull','wcstoumax','wcsxfrm','wctob','wctomb','wctrans','wctype','wmemchr','wmemcmp','wmemcpy','wmemmove','wmemset','wprintf','wscanf'])\n",
    "    \n",
    "\n",
    "    def clang_split(self, i: int, normalized_string: NormalizedString) -> List[NormalizedString]:\n",
    "        ## Tokkenize using clang\n",
    "        tok = []\n",
    "        tu = self.cidx.parse('tmp.cpp',\n",
    "                       args=[''],  \n",
    "                       unsaved_files=[('tmp.cpp', str(normalized_string))],  \n",
    "                       options=0)\n",
    "        for t in tu.get_tokens(extent=tu.cursor.extent):\n",
    "            spelling = t.spelling.strip()\n",
    "            \n",
    "            if spelling == '':\n",
    "                continue\n",
    "                \n",
    "            ## Keyword no need\n",
    "\n",
    "            ## Punctuations no need\n",
    "\n",
    "            ## Literal all to BPE\n",
    "            \n",
    "            spelling = spelling.replace(' ', '')\n",
    "            tok.append(NormalizedString(spelling))\n",
    "\n",
    "        return(tok)\n",
    "    \n",
    "    def pre_tokenize(self, pretok: PreTokenizedString):\n",
    "        pretok.split(self.clang_split)\n",
    "        \n",
    "## Custom tokenizer\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers import normalizers,decoders\n",
    "from tokenizers.normalizers import StripAccents, unicode_normalizer_from_str\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers import processors,pre_tokenizers\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "## Init\n",
    "#my_tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "#my_tokenizer = Tokenizer(BPE())\n",
    "\n",
    "\n",
    "## Load\n",
    "vocab, merges = BPE.read_file(vocab=\"./tokenizer5/v5/draper-vocab.json\", merges=\"./tokenizer5/v5/draper-merges.txt\")\n",
    "my_tokenizer = Tokenizer(BPE(vocab, merges, unk_token=\"<unk>\"))\n",
    "\n",
    "\n",
    "mf = [(\"<s>\",0),(\"<pad>\",1),(\"</s>\",2),(\"<unk>\",3),(\"<mask>\",4)]\n",
    "mf = mf + [('char',5),('int',6),('switch',7),('case',8),('if',9),('break',10),('for',11),('const',12),('unsigned',13),('struct',14),('default',15),('return',16),('long',17),('goto',18),('this',19),('enum',20),('bool',21),('static',22),('false',23),('true',24),('new',25),('delete',26),('while',27),('double',28),('else',29),('private',30),('do',31),('sizeof',32),('void',33),('continue',34),('__attribute__',35),('short',36),('throw',37),('float',38),('register',39),('__FUNCTION__',40),('static_cast',41),('__func__',42),('class',43),('try',44),('dynamic_cast',45),('template',46),('union',47),('reinterpret_cast',48),('catch',49),('operator',50),('const_cast',51),('using',52),('namespace',53),('typename',54),('wchar_t',55),('not',56),('typeof',57),('__label__',58),('__PRETTY_FUNCTION__',59),('auto',60),('__extension__',61),('volatile',62),('__asm__',63),('__volatile__',64),('extern',65),('asm',66),('signed',67),('typedef',68),('typeid',69),('and',70),('or',71),('public',72),('virtual',73),('nullptr',74),('__restrict',75),('__asm',76),('__typeof__',77),('xor',78),('__complex__',79),('__real__',80),('__imag__',81),('not_eq',82),('export',83),('compl',84),('__alignof__',85),('__restrict__',86),('__cdecl',87),('bitor',88),('protected',89),('explicit',90),('friend',91),('decltype',92),('mutable',93),('inline',94),('__const',95),('__stdcall',96),('char16_t',97),('char32_t',98),('_Decimal64',99),('constexpr',100),('bitand',101),('alignof',102),('static_assert',103),('__attribute',104),('thread_local',105),('__alignof',106),('__builtin_va_arg',107),('_Decimal32',108)]\n",
    "mf = mf + [('\\\"',109),('(',110),('*',111),(',',112),(')',113),('{',114),(';',115),('->',116),(':',117),('.',118),('-',119),('=',120),('+',121),('<',122),('++',123),('+=',124),('==',125),('||',126),('!=',127),('}',128),('/',129),('!',130),('>=',131),('[',132),(']',133),('&',134),('::',135),('&&',136),('>',137),('#',138),('--',139),('<=',140),('-=',141),('|',142),('%',143),('?',144),('<<',145),('>>',146),('|=',147),('&=',148),('^',149),('~',150),('^=',151),('...',152),('/=',153),('*=',154),('>>=',155),('<<=',156),('%=',157),('##',158),('->*',159),('\\\\',160),('.*',161),('@',162)]\n",
    "mf = mf + [('_Exit',163),('abs',164),('acos',165),('acosh',166),('asctime',167),('asin',168),('asinh',169),('assert',170),('at_quick_exit',171),('atan',172),('atan2',173),('atanh',174),('atexit',175),('atof',176),('atol',177),('bsearch',178),('btowc',179),('c16rtomb',180),('c32rtomb',181),('cbrt',182),('ceil',183),('cerr',184),('cin',185),('clearerr',186),('clock',187),('clog',188),('copysign',189),('cos',190),('cosh',191),('cout',192),('ctime',193),('difftime',194),('div',195),('errno',196),('exp',197),('exp2',198),('expm1',199),('fabs',200),('fclose',201),('fdim',202),('feclearexcept',203),('fegetenv',204),('fegetexceptflag',205),('fegetround',206),('feholdexcept',207),('feof',208),('feraiseexcept',209),('ferror',210),('fesetenv',211),('fesetexceptflag',212),('fesetround',213),('fetestexcept',214),('feupdateenv',215),('fflush',216),('fgetc',217),('fgetpos',218),('fgets',219),('fgetwc',220),('fgetws',221),('floor',222),('fma',223),('fmax',224),('fmod',225),('fopen',226),('fprintf',227),('fputc',228),('fputs',229),('fputwc',230),('fputws',231),('fread',232),('free',233),('freopen',234),('frexp',235),('fscanf',236),('fseek',237),('fsetpos',238),('ftell',239),('fwide',240),('fwprintf',241),('fwrite',242),('fwscanf',243),('getc',244),('getchar',245),('getenv',246),('gets',247),('getwc',248),('getwchar',249),('gmtime',250),('hypot',251),('ilogb',252),('imaxabs',253),('imaxdiv',254),('isblank',255),('iscntrl',256),('isdigit',257),('isgraph',258),('islower',259),('isprint',260),('ispunct',261),('isspace',262),('isupper',263),('iswalnum',264),('iswalpha',265),('iswblank',266),('iswcntrl',267),('iswctype',268),('iswdigit',269),('iswgraph',270),('iswlower',271),('iswprint',272),('iswpunct',273),('iswspace',274),('iswupper',275),('iswxdigit',276),('isxdigit',277),('labs',278),('ldexp',279),('ldiv',280),('llabs',281),('lldiv',282),('llrint',283),('llround',284),('localeconv',285),('localtime',286),('log',287),('log10',288),('log1p',289),('log2',290),('logb',291),('longjmp',292),('lrint',293),('lround',294),('malloc',295),('mblen',296),('mbrlen',297),('mbrtoc16',298),('mbrtoc32',299),('mbrtowc',300),('mbsinit',301),('mbsrtowcs',302),('mbstowcs',303),('mbtowc',304),('memchr',305),('memcmp',306),('memcpy',307),('memmove',308),('memset',309),('mktime',310),('modf',311),('nan',312),('nearbyint',313),('nextafter',314),('nexttoward',315),('perror',316),('pow',317),('printf',318),('putc',319),('putchar',320),('puts',321),('putwchar',322),('qsort',323),('quick_exit',324),('raise',325),('realloc',326),('remainder',327),('remove',328),('remquo',329),('rename',330),('rewind',331),('rint',332),('round',333),('sca',334),('scalbln',335),('scalbn',336),('setbuf',337),('setjmp',338),('setlocale',339),('setvbuf',340),('signal',341),('sin',342),('sinh',343),('snprintf',344),('sprintf',345),('sqrt',346),('srand',347),('sscanf',348),('strcat',349),('strchr',350),('strcmp',351),('strcoll',352),('strcpy',353),('strcspn',354),('strerror',355),('strftime',356),('strlen',357),('strncat',358),('strncmp',359),('strncpy',360),('strpbrk',361),('strrchr',362),('strspn',363),('strstr',364),('strtod',365),('strtoimax',366),('strtok',367),('strtol',368),('strtoll',369),('strtoull',370),('strtoumax',371),('strxfrm',372),('swprintf',373),('swscanf',374),('tan',375),('tanh',376),('time',377),('tmpfile',378),('tmpnam',379),('tolower',380),('toupper',381),('towctrans',382),('towlower',383),('towupper',384),('trunc',385),('ungetc',386),('ungetwc',387),('vfprintf',388),('vfscanf',389),('vfwprintf',390),('vfwscanf',391),('vprintf',392),('vscanf',393),('vsfscanf',394),('vsnprintf',395),('vsprintf',396),('vsscanf',397),('vswprintf',398),('vwprintf',399),('vwscanf',400),('wcerr',401),('wcin',402),('wclog',403),('wcout',404),('wcrtomb',405),('wcscat',406),('wcschr',407),('wcscmp',408),('wcscoll',409),('wcscpy',410),('wcscspn',411),('wcsftime',412),('wcslne',413),('wcsncat',414),('wcsncmp',415),('wcsncpy',416),('wcspbrk',417),('wcsrchr',418),('wcsrtombs',419),('wcsspn',420),('wcsstr',421),('wcstod',422),('wcstof',423),('wcstoimax',424),('wcstok',425),('wcstol',426),('wcstold',427),('wcstoll',428),('wcstombs',429),('wcstoul',430),('wcstoull',431),('wcstoumax',432),('wcsxfrm',433),('wctob',434),('wctomb',435),('wctrans',436),('wctype',437),('wmemchr',438),('wmemcmp',439),('wmemcpy',440),('wmemmove',441),('wmemset',442),('wprintf',443),('wscanf',444)]\n",
    "\n",
    "my_tokenizer.normalizer = normalizers.Sequence([StripAccents()])\n",
    "my_tokenizer.pre_tokenizer = PreTokenizer.custom(MyTokenizer())\n",
    "my_tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "my_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"<s> $A </s>\",\n",
    "    special_tokens=[\n",
    "    (\"<s>\",0),\n",
    "    (\"<pad>\",1),\n",
    "    (\"</s>\",2),\n",
    "    (\"<unk>\",3),\n",
    "    (\"<mask>\",4)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LATEST Vocab 5\n",
    "from tokenizers.pre_tokenizers import PreTokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers import NormalizedString,PreTokenizedString\n",
    "from typing import List\n",
    "\n",
    "class MyTokenizer:\n",
    "    cidx = cindex.Index.create()\n",
    "    \n",
    "    std_api_calls = set(['_Exit','abs','acos','acosh','asctime','asin','asinh','assert','at_quick_exit','atan','atan2','atanh','atexit','atof','atol','bsearch','btowc','c16rtomb','c32rtomb','cbrt','ceil','cerr','cin','clearerr','clock','clog','copysign','cos','cosh','cout','ctime','difftime','div','errno','exp','exp2','expm1','fabs','fclose','fdim','feclearexcept','fegetenv','fegetexceptflag','fegetround','feholdexcept','feof','feraiseexcept','ferror','fesetenv','fesetexceptflag','fesetround','fetestexcept','feupdateenv','fflush','fgetc','fgetpos','fgets','fgetwc','fgetws','floor','fma','fmax','fmod','fopen','fprintf','fputc','fputs','fputwc','fputws','fread','free','freopen','frexp','fscanf','fseek','fsetpos','ftell','fwide','fwprintf','fwrite','fwscanf','getc','getchar','getenv','gets','getwc','getwchar','gmtime','hypot','ilogb','imaxabs','imaxdiv','isblank','iscntrl','isdigit','isgraph','islower','isprint','ispunct','isspace','isupper','iswalnum','iswalpha','iswblank','iswcntrl','iswctype','iswdigit','iswgraph','iswlower','iswprint','iswpunct','iswspace','iswupper','iswxdigit','isxdigit','labs','ldexp','ldiv','llabs','lldiv','llrint','llround','localeconv','localtime','log','log10','log1p','log2','logb','longjmp','lrint','lround','malloc','mblen','mbrlen','mbrtoc16','mbrtoc32','mbrtowc','mbsinit','mbsrtowcs','mbstowcs','mbtowc','memchr','memcmp','memcpy','memmove','memset','mktime','modf','nan','nearbyint','nextafter','nexttoward','perror','pow','printf','putc','putchar','puts','putwchar','qsort','quick_exit','raise','realloc','remainder','remove','remquo','rename','rewind','rint','round','sca','scalbln','scalbn','setbuf','setjmp','setlocale','setvbuf','signal','sin','sinh','snprintf','sprintf','sqrt','srand','sscanf','strcat','strchr','strcmp','strcoll','strcpy','strcspn','strerror','strftime','strlen','strncat','strncmp','strncpy','strpbrk','strrchr','strspn','strstr','strtod','strtoimax','strtok','strtol','strtoll','strtoull','strtoumax','strxfrm','swprintf','swscanf','tan','tanh','time','tmpfile','tmpnam','tolower','toupper','towctrans','towlower','towupper','trunc','ungetc','ungetwc','vfprintf','vfscanf','vfwprintf','vfwscanf','vprintf','vscanf','vsfscanf','vsnprintf','vsprintf','vsscanf','vswprintf','vwprintf','vwscanf','wcerr','wcin','wclog','wcout','wcrtomb','wcscat','wcschr','wcscmp','wcscoll','wcscpy','wcscspn','wcsftime','wcslne','wcsncat','wcsncmp','wcsncpy','wcspbrk','wcsrchr','wcsrtombs','wcsspn','wcsstr','wcstod','wcstof','wcstoimax','wcstok','wcstol','wcstold','wcstoll','wcstombs','wcstoul','wcstoull','wcstoumax','wcsxfrm','wctob','wctomb','wctrans','wctype','wmemchr','wmemcmp','wmemcpy','wmemmove','wmemset','wprintf','wscanf'])\n",
    "    \n",
    "\n",
    "    def clang_split(self, i: int, normalized_string: NormalizedString) -> List[NormalizedString]:\n",
    "        ## Tokkenize using clang\n",
    "        tok = []\n",
    "        tu = self.cidx.parse('tmp.c',\n",
    "                       args=[''],  \n",
    "                       unsaved_files=[('tmp.c', str(normalized_string))],  \n",
    "                       options=0)\n",
    "        for t in tu.get_tokens(extent=tu.cursor.extent):\n",
    "            spelling = t.spelling.strip()\n",
    "            \n",
    "            if spelling == '':\n",
    "                continue\n",
    "                \n",
    "            ## Keyword no need\n",
    "\n",
    "            ## Punctuations no need\n",
    "\n",
    "            ## Literal all to BPE\n",
    "            \n",
    "            #spelling = spelling.replace(' ', '')\n",
    "            tok.append(NormalizedString(spelling))\n",
    "\n",
    "        return(tok)\n",
    "    \n",
    "    def pre_tokenize(self, pretok: PreTokenizedString):\n",
    "        pretok.split(self.clang_split)\n",
    "        \n",
    "## Custom tokenizer\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers import normalizers,decoders\n",
    "from tokenizers.normalizers import StripAccents, unicode_normalizer_from_str, Replace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers import processors,pre_tokenizers\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "## Init\n",
    "#my_tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "#my_tokenizer = Tokenizer(BPE())\n",
    "\n",
    "\n",
    "## Load\n",
    "vocab, merges = BPE.read_file(vocab=\"./tokenizer5/v5_drapgh/drapgh-vocab.json\", merges=\"./tokenizer5/v5_drapgh/drapgh-merges.txt\")\n",
    "my_tokenizer = Tokenizer(BPE(vocab, merges, unk_token=\"<unk>\"))\n",
    "\n",
    "\n",
    "mf = [(\"<s>\",0),(\"<pad>\",1),(\"</s>\",2),(\"<unk>\",3),(\"<mask>\",4)]\n",
    "mf = mf + [('char',5),('int',6),('switch',7),('case',8),('if',9),('break',10),('for',11),('const',12),('unsigned',13),('struct',14),('default',15),('return',16),('long',17),('goto',18),('this',19),('enum',20),('bool',21),('static',22),('false',23),('true',24),('new',25),('delete',26),('while',27),('double',28),('else',29),('private',30),('do',31),('sizeof',32),('void',33),('continue',34),('__attribute__',35),('short',36),('throw',37),('float',38),('register',39),('__FUNCTION__',40),('static_cast',41),('__func__',42),('class',43),('try',44),('dynamic_cast',45),('template',46),('union',47),('reinterpret_cast',48),('catch',49),('operator',50),('const_cast',51),('using',52),('namespace',53),('typename',54),('wchar_t',55),('not',56),('typeof',57),('__label__',58),('__PRETTY_FUNCTION__',59),('auto',60),('__extension__',61),('volatile',62),('__asm__',63),('__volatile__',64),('extern',65),('asm',66),('signed',67),('typedef',68),('typeid',69),('and',70),('or',71),('public',72),('virtual',73),('nullptr',74),('__restrict',75),('__asm',76),('__typeof__',77),('xor',78),('__complex__',79),('__real__',80),('__imag__',81),('not_eq',82),('export',83),('compl',84),('__alignof__',85),('__restrict__',86),('__cdecl',87),('bitor',88),('protected',89),('explicit',90),('friend',91),('decltype',92),('mutable',93),('inline',94),('__const',95),('__stdcall',96),('char16_t',97),('char32_t',98),('_Decimal64',99),('constexpr',100),('bitand',101),('alignof',102),('static_assert',103),('__attribute',104),('thread_local',105),('__alignof',106),('__builtin_va_arg',107),('_Decimal32',108)]\n",
    "mf = mf + [('\\\"',109),('(',110),('*',111),(',',112),(')',113),('{',114),(';',115),('->',116),(':',117),('.',118),('-',119),('=',120),('+',121),('<',122),('++',123),('+=',124),('==',125),('||',126),('!=',127),('}',128),('/',129),('!',130),('>=',131),('[',132),(']',133),('&',134),('::',135),('&&',136),('>',137),('#',138),('--',139),('<=',140),('-=',141),('|',142),('%',143),('?',144),('<<',145),('>>',146),('|=',147),('&=',148),('^',149),('~',150),('^=',151),('...',152),('/=',153),('*=',154),('>>=',155),('<<=',156),('%=',157),('##',158),('->*',159),('\\\\',160),('.*',161),('@',162)]\n",
    "mf = mf + [('_Exit',163),('abs',164),('acos',165),('acosh',166),('asctime',167),('asin',168),('asinh',169),('assert',170),('at_quick_exit',171),('atan',172),('atan2',173),('atanh',174),('atexit',175),('atof',176),('atol',177),('bsearch',178),('btowc',179),('c16rtomb',180),('c32rtomb',181),('cbrt',182),('ceil',183),('cerr',184),('cin',185),('clearerr',186),('clock',187),('clog',188),('copysign',189),('cos',190),('cosh',191),('cout',192),('ctime',193),('difftime',194),('div',195),('errno',196),('exp',197),('exp2',198),('expm1',199),('fabs',200),('fclose',201),('fdim',202),('feclearexcept',203),('fegetenv',204),('fegetexceptflag',205),('fegetround',206),('feholdexcept',207),('feof',208),('feraiseexcept',209),('ferror',210),('fesetenv',211),('fesetexceptflag',212),('fesetround',213),('fetestexcept',214),('feupdateenv',215),('fflush',216),('fgetc',217),('fgetpos',218),('fgets',219),('fgetwc',220),('fgetws',221),('floor',222),('fma',223),('fmax',224),('fmod',225),('fopen',226),('fprintf',227),('fputc',228),('fputs',229),('fputwc',230),('fputws',231),('fread',232),('free',233),('freopen',234),('frexp',235),('fscanf',236),('fseek',237),('fsetpos',238),('ftell',239),('fwide',240),('fwprintf',241),('fwrite',242),('fwscanf',243),('getc',244),('getchar',245),('getenv',246),('gets',247),('getwc',248),('getwchar',249),('gmtime',250),('hypot',251),('ilogb',252),('imaxabs',253),('imaxdiv',254),('isblank',255),('iscntrl',256),('isdigit',257),('isgraph',258),('islower',259),('isprint',260),('ispunct',261),('isspace',262),('isupper',263),('iswalnum',264),('iswalpha',265),('iswblank',266),('iswcntrl',267),('iswctype',268),('iswdigit',269),('iswgraph',270),('iswlower',271),('iswprint',272),('iswpunct',273),('iswspace',274),('iswupper',275),('iswxdigit',276),('isxdigit',277),('labs',278),('ldexp',279),('ldiv',280),('llabs',281),('lldiv',282),('llrint',283),('llround',284),('localeconv',285),('localtime',286),('log',287),('log10',288),('log1p',289),('log2',290),('logb',291),('longjmp',292),('lrint',293),('lround',294),('malloc',295),('mblen',296),('mbrlen',297),('mbrtoc16',298),('mbrtoc32',299),('mbrtowc',300),('mbsinit',301),('mbsrtowcs',302),('mbstowcs',303),('mbtowc',304),('memchr',305),('memcmp',306),('memcpy',307),('memmove',308),('memset',309),('mktime',310),('modf',311),('nan',312),('nearbyint',313),('nextafter',314),('nexttoward',315),('perror',316),('pow',317),('printf',318),('putc',319),('putchar',320),('puts',321),('putwchar',322),('qsort',323),('quick_exit',324),('raise',325),('realloc',326),('remainder',327),('remove',328),('remquo',329),('rename',330),('rewind',331),('rint',332),('round',333),('sca',334),('scalbln',335),('scalbn',336),('setbuf',337),('setjmp',338),('setlocale',339),('setvbuf',340),('signal',341),('sin',342),('sinh',343),('snprintf',344),('sprintf',345),('sqrt',346),('srand',347),('sscanf',348),('strcat',349),('strchr',350),('strcmp',351),('strcoll',352),('strcpy',353),('strcspn',354),('strerror',355),('strftime',356),('strlen',357),('strncat',358),('strncmp',359),('strncpy',360),('strpbrk',361),('strrchr',362),('strspn',363),('strstr',364),('strtod',365),('strtoimax',366),('strtok',367),('strtol',368),('strtoll',369),('strtoull',370),('strtoumax',371),('strxfrm',372),('swprintf',373),('swscanf',374),('tan',375),('tanh',376),('time',377),('tmpfile',378),('tmpnam',379),('tolower',380),('toupper',381),('towctrans',382),('towlower',383),('towupper',384),('trunc',385),('ungetc',386),('ungetwc',387),('vfprintf',388),('vfscanf',389),('vfwprintf',390),('vfwscanf',391),('vprintf',392),('vscanf',393),('vsfscanf',394),('vsnprintf',395),('vsprintf',396),('vsscanf',397),('vswprintf',398),('vwprintf',399),('vwscanf',400),('wcerr',401),('wcin',402),('wclog',403),('wcout',404),('wcrtomb',405),('wcscat',406),('wcschr',407),('wcscmp',408),('wcscoll',409),('wcscpy',410),('wcscspn',411),('wcsftime',412),('wcslne',413),('wcsncat',414),('wcsncmp',415),('wcsncpy',416),('wcspbrk',417),('wcsrchr',418),('wcsrtombs',419),('wcsspn',420),('wcsstr',421),('wcstod',422),('wcstof',423),('wcstoimax',424),('wcstok',425),('wcstol',426),('wcstold',427),('wcstoll',428),('wcstombs',429),('wcstoul',430),('wcstoull',431),('wcstoumax',432),('wcsxfrm',433),('wctob',434),('wctomb',435),('wctrans',436),('wctype',437),('wmemchr',438),('wmemcmp',439),('wmemcpy',440),('wmemmove',441),('wmemset',442),('wprintf',443),('wscanf',444)]\n",
    "\n",
    "my_tokenizer.normalizer = normalizers.Sequence([StripAccents(), Replace(\" \", \"Ä\")])\n",
    "my_tokenizer.pre_tokenizer = PreTokenizer.custom(MyTokenizer())\n",
    "my_tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "my_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"<s> $A </s>\",\n",
    "    special_tokens=[\n",
    "    (\"<s>\",0),\n",
    "    (\"<pad>\",1),\n",
    "    (\"</s>\",2),\n",
    "    (\"<unk>\",3),\n",
    "    (\"<mask>\",4)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=my_tokenizer.encode_batch([\"\"\"static void v9fs_open(void *opaque) int32_t fid ; V9fsQID qid ; size_t offset = 7 ; struct stat stbuf ; V9fsFidState * fidp ; V9fsPDU * pdu = opaque ; V9fsState * s = pdu -> s ; if ( s -> proto_version == V9FS_PROTO_2000L )  err = pdu_unmarshal ( pdu , offset , \"dd\" , & fid , & mode ); err = pdu_unmarshal ( pdu , offset , \"db\" , & fid , & modebyte ); if ( err < 0 )  fidp = get_fid ( pdu , fid ); static V9fsFidState *get_fid(V9fsPDU *pdu, int32_t fid) int err ; V9fsFidState * f ; V9fsState * s = pdu -> s ; for (f = s->fid_list; f; f = f->next) if ( f -> fid == fid )  f -> ref ++; err = v9fs_reopen_fid ( pdu , f ); if ( err < 0 )  return NULL ; return f ; return NULL ; if ( fidp == NULL )  err = v9fs_co_lstat ( pdu , & fidp -> path , & stbuf ); if ( err < 0 )  stat_to_qid ( & stbuf , & qid ); static void stat_to_qid(const struct stat *stbuf, V9fsQID *qidp) memset ( & qidp -> path , 0 , sizeof ( qidp -> path ) ); size = MIN ( sizeof ( stbuf -> st_ino ) , sizeof ( qidp -> path ) ); memcpy ( & qidp -> path , & stbuf -> st_ino , size ); qidp -> version = stbuf -> st_mtime ^ ( stbuf -> st_size << 8 ); qidp -> type = 0; qidp -> type |= P9_QID_TYPE_DIR; qidp -> type |= P9_QID_TYPE_SYMLINK; \"\"\"])\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer.decode(k[0].ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer.decode(k[0].ids).replace('Ä',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPARE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_ONLY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydataset = 'd2a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer.enable_truncation(max_length=1024)\n",
    "my_tokenizer.enable_padding(direction='right', pad_id=1, pad_type_id=0, pad_token='<pad>', length=None, pad_to_multiple_of=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(code):\n",
    "    ## Remove code comments\n",
    "    pat = re.compile(r'(/\\*([^*]|(\\*+[^*/]))*\\*+/)|(//.*)')\n",
    "    code = re.sub(pat,'',code)\n",
    "    code = re.sub('\\n','',code)\n",
    "    code = re.sub('\\t','',code)\n",
    "    return(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encodings(encodings):\n",
    "    input_ids=[]\n",
    "    attention_mask=[]\n",
    "    for enc in encodings:\n",
    "        input_ids.append(enc.ids)\n",
    "        attention_mask.append(enc.attention_mask)\n",
    "    return {'input_ids':input_ids, 'attention_mask':attention_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_colname(x):\n",
    "    try:\n",
    "        x = x.rename(columns={'functionSource': \"func\"})\n",
    "    except:\n",
    "        None\n",
    "\n",
    "    try:\n",
    "        x = x.rename(columns={'code': \"func\"})\n",
    "    except:\n",
    "        None\n",
    "\n",
    "    try:\n",
    "        x = x.rename(columns={'label': \"target\"})\n",
    "    except:\n",
    "        None\n",
    "    return(x)\n",
    "\n",
    "if mydataset =='devign':\n",
    "    if TEST_ONLY:\n",
    "        \n",
    "        test_index=set()\n",
    "        with open('data/devign/test.txt') as f:\n",
    "            for line in f:\n",
    "                line=line.strip()\n",
    "                test_index.add(int(line))\n",
    "        mydata = pd.read_json('data/devign/Devign.json')\n",
    "        m3=mydata.iloc[list(test_index)]\n",
    "        mydata = None\n",
    "        del(mydata)\n",
    "        \n",
    "    else:\n",
    "        train_index=set()\n",
    "        valid_index=set()\n",
    "        test_index=set()\n",
    "\n",
    "        with open('data/devign/train.txt') as f:\n",
    "            for line in f:\n",
    "                line=line.strip()\n",
    "                train_index.add(int(line))\n",
    "\n",
    "        with open('data/devign/valid.txt') as f:\n",
    "            for line in f:\n",
    "                line=line.strip()\n",
    "                valid_index.add(int(line))\n",
    "\n",
    "        with open('data/devign/test.txt') as f:\n",
    "            for line in f:\n",
    "                line=line.strip()\n",
    "                test_index.add(int(line))\n",
    "\n",
    "        mydata = pd.read_json('data/devign/Devign.json')\n",
    "        m1=mydata.iloc[list(train_index)]\n",
    "        m2=mydata.iloc[list(valid_index)]\n",
    "        m3=mydata.iloc[list(test_index)]\n",
    "\n",
    "        mydata = None\n",
    "        del(mydata)\n",
    "    \n",
    "\n",
    "elif mydataset =='d2a':\n",
    "    task = 'function'\n",
    "    \n",
    "    if TEST_ONLY:\n",
    "        m3 = pd.read_csv('data/%s/DAX_D2ALBData/%s/d2a_lbv1_%s_dev.csv'%(mydataset,task,task))\n",
    "        m3 = replace_colname(m3)\n",
    "    else:\n",
    "        m1 = pd.read_csv('data/%s/DAX_D2ALBData/%s/d2a_lbv1_%s_train.csv'%(mydataset,task,task))\n",
    "        m2 = pd.read_csv('data/%s/DAX_D2ALBData/%s/d2a_lbv1_%s_dev.csv'%(mydataset,task,task))\n",
    "        m3 = pd.read_csv('data/%s/DAX_D2ALBData/%s/d2a_lbv1_%s_test.csv'%(mydataset,task,task))\n",
    "       \n",
    "        m1 = replace_colname(m1)\n",
    "        m2 = replace_colname(m2)\n",
    "        m3 = replace_colname(m3)\n",
    "        \n",
    "        \n",
    "else:\n",
    "    \n",
    "    def replace_colname(x):\n",
    "        try:\n",
    "            x = x.rename(columns={'functionSource': \"func\"})\n",
    "        except:\n",
    "            None\n",
    "            \n",
    "        try:\n",
    "            x = x.rename(columns={'code': \"func\"})\n",
    "        except:\n",
    "            None\n",
    "\n",
    "        try:\n",
    "            x = x.rename(columns={'label': \"target\"})\n",
    "        except:\n",
    "            None\n",
    "        return(x)\n",
    "    \n",
    "    \n",
    "    if TEST_ONLY:\n",
    "        m3 = pd.read_pickle('data/%s/%s_test.pkl'%(mydataset,mydataset))\n",
    "        m3 = replace_colname(m3)\n",
    "        \n",
    "    else:\n",
    "        m1 = pd.read_pickle('data/%s/%s_train.pkl'%(mydataset,mydataset))\n",
    "        m2 = pd.read_pickle('data/%s/%s_val.pkl'%(mydataset,mydataset))\n",
    "        m3 = pd.read_pickle('data/%s/%s_test.pkl'%(mydataset,mydataset))\n",
    "\n",
    "        m1 = replace_colname(m1)\n",
    "        m2 = replace_colname(m2)\n",
    "        m3 = replace_colname(m3)\n",
    "\n",
    "if TEST_ONLY:\n",
    "    m3.func = m3.func.apply(cleaner)\n",
    "    test_encodings = my_tokenizer.encode_batch(m3.func)\n",
    "    try:\n",
    "        test_encodings = [{'func':enc.ids,'target':lab} for enc,lab in zip(test_encodings,m3.target.tolist())]\n",
    "    except:\n",
    "        test_encodings = [{'func':enc.ids,'target':lab} for enc,lab in zip(test_encodings,(m3['combine']*1).tolist())]\n",
    "\n",
    "else:\n",
    "    \n",
    "    m1.func = m1.func.apply(cleaner)\n",
    "    train_encodings = my_tokenizer.encode_batch(m1.func)\n",
    "    try:\n",
    "        train_encodings = [{'func':enc.ids,'target':lab} for enc,lab in zip(train_encodings,m1.target.tolist())]\n",
    "    except:\n",
    "        train_encodings = [{'func':enc.ids,'target':lab} for enc,lab in zip(train_encodings,(m1['combine']*1).tolist())]\n",
    "\n",
    "\n",
    "    m2.func = m2.func.apply(cleaner)\n",
    "    val_encodings = my_tokenizer.encode_batch(m2.func)\n",
    "    try:\n",
    "        val_encodings = [{'func':enc.ids,'target':lab} for enc,lab in zip(val_encodings,m2.target.tolist())]\n",
    "    except:\n",
    "        val_encodings = [{'func':enc.ids,'target':lab} for enc,lab in zip(val_encodings,(m2['combine']*1).tolist())]\n",
    "\n",
    "        \n",
    "    m3.func = m3.func.apply(cleaner)\n",
    "    test_encodings = my_tokenizer.encode_batch(m3.func)\n",
    "    try:\n",
    "        test_encodings = [{'func':enc.ids,'target':lab} for enc,lab in zip(test_encodings,m3.target.tolist())]\n",
    "    except:\n",
    "        test_encodings = [{'func':enc.ids,'target':lab} for enc,lab in zip(test_encodings,(m3['combine']*1).tolist())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODES = torchtext.data.Field(batch_first=True, fix_length=1024,use_vocab=False)\n",
    "LABEL = torchtext.data.LabelField(dtype=torch.long, is_target=True,use_vocab=False)\n",
    "fields = {'func': ('codes', CODES), 'target': ('label', LABEL)}\n",
    "\n",
    "class TabularDataset_From_List(torchtext.data.Dataset):\n",
    "    def __init__(self, input_list, format, fields, skip_header=False, **kwargs):\n",
    "        make_example = {\n",
    "            'json': torchtext.data.Example.fromJSON, 'dict': torchtext.data.Example.fromdict}[format.lower()]\n",
    "\n",
    "        examples = [make_example(item, fields) for item in input_list]\n",
    "\n",
    "        if make_example in (torchtext.data.Example.fromdict, torchtext.data.Example.fromJSON):\n",
    "            fields, field_dict = [], fields\n",
    "            for field in field_dict.values():\n",
    "                if isinstance(field, list):\n",
    "                    fields.extend(field)\n",
    "                else:\n",
    "                    fields.append(field)\n",
    "\n",
    "        super(TabularDataset_From_List, self).__init__(examples, fields, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, path=None, root='.data', train=None, validation=None,\n",
    "               test=None, **kwargs):\n",
    "        if path is None:\n",
    "            path = cls.download(root)\n",
    "        train_data = None if train is None else cls(\n",
    "            train, **kwargs)\n",
    "        val_data = None if validation is None else cls(\n",
    "            validation, **kwargs)\n",
    "        test_data = None if test is None else cls(\n",
    "            test, **kwargs)\n",
    "        return tuple(d for d in (train_data, val_data, test_data)\n",
    "                     if d is not None)\n",
    "\n",
    "\n",
    "## Import the 100K data as TabularDataset\n",
    "\n",
    "if TEST_ONLY:\n",
    "    test_data = TabularDataset_From_List(test_encodings,'dict',fields = fields)\n",
    "else:\n",
    "    train_data = TabularDataset_From_List(train_encodings,'dict',fields = fields)\n",
    "    val_data = TabularDataset_From_List(val_encodings,'dict',fields = fields)\n",
    "    test_data = TabularDataset_From_List(test_encodings,'dict',fields = fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IF ITERABLE DATASETTEST_ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(IterableDataset):\n",
    "    \n",
    "    def __init__(self,filename,rcount):\n",
    "     \n",
    "        self.filename=filename\n",
    "        self.len_labels=rcount\n",
    "        super().__init__()\n",
    "                    \n",
    "    def process(self,filename):\n",
    "        import pickle \n",
    "        with open(filename, \"rb\") as f:\n",
    "            while True:\n",
    "                try:\n",
    "                    item = pickle.load(f)\n",
    "                    yield {'input_ids': torch.tensor(item['input_ids']), 'attention_mask':torch.tensor(item['attention_mask']), 'labels':torch.tensor(item['labels'])}\n",
    "                except EOFError:\n",
    "                    break\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return self.len_labels\n",
    "\n",
    "    def __iter__(self):\n",
    "        dataset=self.process(self.filename)          \n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rcount = len(pd.read_pickle('data/draper/draper_train.pkl'))\n",
    "train_dataset = MyDataset('data/draper/draper_stream_train.pkl', train_rcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_rcount = len(pd.read_pickle('data/draper/draper_val.pkl'))\n",
    "val_dataset = MyDataset('data/draper/draper_stream_val.pkl', val_rcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rcount = len(pd.read_pickle('data/draper/draper_test.pkl'))\n",
    "test_dataset = MyDataset('data/draper/draper_stream_test.pkl', test_rcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END ITERABLE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = VOCAB_SIZE\n",
    "\n",
    "# place into iterators\n",
    "\n",
    "if TEST_ONLY:\n",
    "    test_iterator = torchtext.data.BucketIterator(\n",
    "        test_data, \n",
    "        batch_size = 1,\n",
    "        sort = False,\n",
    "        shuffle = False)\n",
    "    \n",
    "else:\n",
    "    train_iterator, valid_iterator, test_iterator = torchtext.data.BucketIterator.splits(\n",
    "        (train_data, val_data, test_data), \n",
    "        batch_size = BATCH_SIZE,\n",
    "        sort = False,\n",
    "        shuffle = False)\n",
    "\n",
    "UNK_IDX = 3\n",
    "PAD_IDX = 1\n",
    "\n",
    "# test_iterator = torchtext.data.BucketIterator(\n",
    "#     test_data, \n",
    "#     batch_size = BATCH_SIZE,\n",
    "#     sort = False,\n",
    "#     shuffle = False)\n",
    "\n",
    "#from torch.utils.data import DataLoader\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset, shuffle=False, batch_size=BATCH_SIZE)\n",
    "# val_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=BATCH_SIZE)\n",
    "# test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCNN(nn.Module):\n",
    "    def __init__(self, EMBED_SIZE, EMBED_DIM):\n",
    "        super(myCNN,self).__init__()\n",
    "        \n",
    "        pretrained_weights = RobertaModel.from_pretrained('./models/v5/VulBERTa_base_clangBPEcustVocab5_1024posEmbed_drapgh/checkpoint-580000/').embeddings.word_embeddings.weight\n",
    "\n",
    "        self.embed = nn.Embedding.from_pretrained(pretrained_weights,\n",
    "                                                  freeze=True,\n",
    "                                                  padding_idx=1)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=EMBED_DIM, out_channels=200, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(in_channels=EMBED_DIM, out_channels=200, kernel_size=4)\n",
    "        self.conv3 = nn.Conv1d(in_channels=EMBED_DIM, out_channels=200, kernel_size=5)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc1 = nn.Linear(200*3,256) #500\n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.fc3 = nn.Linear(128,2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = x.permute(0,2,1)\n",
    "\n",
    "        x1 = F.relu(self.conv1(x))\n",
    "        x2 = F.relu(self.conv2(x))\n",
    "        x3 = F.relu(self.conv3(x))\n",
    "        \n",
    "        x1 = F.max_pool1d(x1, x1.shape[2])\n",
    "        x2 = F.max_pool1d(x2, x2.shape[2])\n",
    "        x3 = F.max_pool1d(x3, x3.shape[2])\n",
    "        \n",
    "        x = torch.cat([x1,x2,x3],dim=1)\n",
    "        \n",
    "        # flatten the tensor\n",
    "        x = x.flatten(1)\n",
    "        \n",
    "        # apply mean over the last dimension\n",
    "        #x = torch.mean(x, -1)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return(x)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myLSTM(nn.Module):\n",
    "    def __init__(self, EMBED_SIZE, EMBED_DIM):\n",
    "        super(myLSTM,self).__init__()\n",
    "\n",
    "        pretrained_weights = RobertaModel.from_pretrained('./models/v5/VulBERTa_base_clangBPEcustVocab5_1024posEmbed_drapgh/checkpoint-580000/').embeddings.word_embeddings.weight\n",
    "\n",
    "        self.embed = nn.Embedding.from_pretrained(pretrained_weights,\n",
    "                                                  freeze=True,\n",
    "                                                  padding_idx=1)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=EMBED_DIM, \n",
    "                            hidden_size = 256, \n",
    "                            bidirectional = True,\n",
    "                            num_layers = 2)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc1 = nn.Linear(256*2,256) #500\n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.fc3 = nn.Linear(128,2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        \n",
    "        x = x.permute(1,0,2)\n",
    "    \n",
    "        output, (hidden, cell) = self.lstm(x)\n",
    "        x = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "            \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = myCNN(EMBED_SIZE,EMBED_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = myLSTM(EMBED_SIZE,EMBED_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.embed.weight.data[UNK_IDX] = torch.zeros(EMBED_DIM)\n",
    "model.embed.weight.data[PAD_IDX] = torch.zeros(EMBED_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if multigpu:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Num of trainable param: ',sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "## Define loss function\n",
    "#criterion = nn.BCELoss().to(device) ## Sigmoid activation function\n",
    "#criterion = nn.NLLLoss().to(device) ### Log_softmax activation\n",
    "#weights = torch.tensor([1.0,3.5])\n",
    "#print(list(train_data.label))\n",
    "\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss(weight=weights) ## No activation function in model bcs softmax included\n",
    "#criterion = nn.BCELoss() ## with Sigmoid to pair\n",
    "\n",
    "import sklearn\n",
    "#cw = sklearn.utils.class_weight.compute_class_weight(class_weight='balanced',classes=[0,1],y=pd.read_pickle('data/draper/draper_train.pkl')['combine']*1)\n",
    "#cw = sklearn.utils.class_weight.compute_class_weight(class_weight='balanced',classes=[0,1],y=m1.label.tolist())\n",
    "#c_weights = torch.FloatTensor([cw[0], cw[1]])\n",
    "#c_weights = torch.FloatTensor([1, 5.5])\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss(weight=c_weights)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_accuracy(probs,all_labels):\n",
    "    def getClass(x):\n",
    "        return(x.index(max(x)))\n",
    "    \n",
    "    all_labels = all_labels.tolist()\n",
    "    probs = pd.Series(probs.tolist())\n",
    "    all_predicted = probs.apply(getClass)\n",
    "    all_predicted.reset_index(drop=True, inplace=True)\n",
    "    vc = pd.value_counts(all_predicted == all_labels)\n",
    "    try:\n",
    "        acc = vc[1]/len(all_labels)\n",
    "    except:\n",
    "        if(vc.index[0]==False):\n",
    "            acc = 0\n",
    "        else:\n",
    "            acc = 1\n",
    "    return(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training started.....')\n",
    "\n",
    "EPOCHS=20\n",
    "BEST_VAL = 9999.9\n",
    "BEST_MODEL = None\n",
    "BEST_EPOCH = None\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    running_acc = 0\n",
    "    running_loss = 0\n",
    "    timer = time.time()\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_iterator:\n",
    "        batch.codes, batch.label = batch.codes.to(device), batch.label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch.codes)\n",
    "        loss = criterion(output, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = softmax_accuracy(output,batch.label)\n",
    "        running_acc += acc\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        running_acc_val = 0\n",
    "        running_loss_val = 0\n",
    "        for batch in valid_iterator:\n",
    "            batch.codes, batch.label = batch.codes.to(device), batch.label.to(device)\n",
    "            output_val = model(batch.codes)\n",
    "            loss_val = criterion(output_val,batch.label)\n",
    "            acc_val = softmax_accuracy(output_val,batch.label)\n",
    "            running_acc_val += acc_val\n",
    "            running_loss_val += loss_val.item()\n",
    "\n",
    "    print_out = \"Epoch %d - Training acc: %.4f -Training loss: %.4f - Val acc: %.4f - Val loss: %.4f - Time: %.4fs \\n\" % (e+1,\n",
    "    running_acc/len(train_iterator),\n",
    "    running_loss/len(train_iterator),\n",
    "    running_acc_val/len(valid_iterator),\n",
    "    running_loss_val/len(valid_iterator),\n",
    "    (time.time()-timer))\n",
    "    \n",
    "    \n",
    "    selected_model = False\n",
    "    \n",
    "    if selected_model:\n",
    "        \n",
    "        myfile = open(\"res.txt\", \"a\")\n",
    "\n",
    "        if (running_loss_val/len(valid_iterator)) < BEST_VAL:\n",
    "            print('Val_loss decreased!')\n",
    "            print(print_out, end='')\n",
    "            myfile.write('Val_loss decreased!')\n",
    "            myfile.write(print_out)\n",
    "\n",
    "            BEST_VAL = (running_loss_val/len(valid_iterator))\n",
    "            BEST_MODEL = copy.deepcopy(model)\n",
    "            BEST_EPOCH = e+1\n",
    "            model_name = 'models/cnn_v5_voc5_pretraindrapgh_devign_run1/model_ep_%d.tar' % (e+1)\n",
    "            torch.save({\n",
    "                'epoch': e+1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss}, model_name)\n",
    "\n",
    "        else:\n",
    "            print(print_out, end='')\n",
    "            myfile.write(print_out)\n",
    "\n",
    "        myfile.close()\n",
    "        \n",
    "    else:\n",
    "        print(print_out, end='')\n",
    "        model_name = 'models/cnn_v5_voc5_pretraindrapgh_devign_run1/model_ep_%d.tar' % (e+1)\n",
    "        torch.save({\n",
    "            'epoch': e+1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss}, model_name)\n",
    "\n",
    "        \n",
    "\n",
    "print('Training completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_testing(all_pred, all_labels):\n",
    "    def getClass(x):\n",
    "        return(x.index(max(x)))\n",
    "\n",
    "    probs = pd.Series(all_pred)\n",
    "    all_predicted = probs.apply(getClass)\n",
    "    all_predicted.reset_index(drop=True, inplace=True)\n",
    "    vc = pd.value_counts(all_predicted == all_labels)\n",
    "\n",
    "    probs2=[]\n",
    "    for x in probs:\n",
    "        probs2.append(x[1])\n",
    "\n",
    "    confusion = sklearn.metrics.confusion_matrix(y_true=all_labels, y_pred=all_predicted)\n",
    "    print('Confusion matrix: \\n',confusion)\n",
    "\n",
    "    try:\n",
    "        tn, fp, fn, tp = confusion.ravel()\n",
    "        print('\\nTP:',tp)\n",
    "        print('FP:',fp)\n",
    "        print('TN:',tn)\n",
    "        print('FN:',fn)\n",
    "\n",
    "        ## Performance measure\n",
    "        print('\\nAccuracy: '+ str(sklearn.metrics.accuracy_score(y_true=all_labels, y_pred=all_predicted)))\n",
    "        print('Precision: '+ str(sklearn.metrics.precision_score(y_true=all_labels, y_pred=all_predicted)))\n",
    "        print('F-measure: '+ str(sklearn.metrics.f1_score(y_true=all_labels, y_pred=all_predicted)))\n",
    "        print('Recall: '+ str(sklearn.metrics.recall_score(y_true=all_labels, y_pred=all_predicted)))\n",
    "        print('Precision-Recall AUC: '+ str(sklearn.metrics.average_precision_score(y_true=all_labels, y_score=probs2)))\n",
    "        print('AUC: '+ str(sklearn.metrics.roc_auc_score(y_true=all_labels, y_score=probs2)))\n",
    "        print('MCC: '+ str(sklearn.metrics.matthews_corrcoef(y_true=all_labels, y_pred=all_predicted)))\n",
    "    except:\n",
    "        None\n",
    "        print('This is multiclass prediction')\n",
    "    return(all_predicted)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing started.......')\n",
    "## Testing\n",
    "checkpoint = torch.load('models/cnn_v5_voc5_pretraindrapgh_d2a_fixed/model_ep_2.tar', map_location='cuda')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    running_acc_test = 0\n",
    "    running_loss_test = 0\n",
    "    all_pred=[]\n",
    "    all_labels=[]\n",
    "    for batch in test_iterator:\n",
    "        batch.codes, batch.label = batch.codes.to(device), batch.label.to(device)\n",
    "        output_test = model(batch.codes).squeeze(1)\n",
    "        loss_test = criterion(output_test,batch.label)\n",
    "        acc_test = softmax_accuracy(output_test,batch.label)\n",
    "        running_acc_test += acc_test\n",
    "        running_loss_test += loss_test.item()\n",
    "        all_pred += output_test.tolist()\n",
    "        all_labels += batch.label.tolist()\n",
    "\n",
    "ap=evaluate_testing(all_pred, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn=['non-vulnerable','CWE-404','CWE-476','CWE-119','CWE-706','CWE-670','CWE-673','CWE-119, CWE-666, CWE-573','CWE-573','CWE-668','CWE-400, CWE-665, CWE-020','CWE-662','CWE-400','CWE-665','CWE-020','CWE-074','CWE-362','CWE-191','CWE-190','CWE-610','CWE-704','CWE-170','CWE-676','CWE-187','CWE-138','CWE-369','CWE-662, CWE-573','CWE-834','CWE-400, CWE-665','CWE-400, CWE-404','CWE-221','CWE-754','CWE-311','CWE-404, CWE-668','CWE-506','CWE-758','CWE-666','CWE-467','CWE-327','CWE-666, CWE-573','CWE-469']\n",
    "report = sklearn.metrics.classification_report(y_true=all_labels, y_pred=ap, digits=6,labels=np.arange(0,41),target_names=tn)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = sklearn.metrics.confusion_matrix(y_true=[1 if x == 0 else 0 for x in all_labels], y_pred=[1 if x == 0 else 0 for x in ap])\n",
    "tn, fp, fn, tp = confusion.ravel()\n",
    "print('\\nTP:',tp)\n",
    "print('FP:',fp)\n",
    "print('TN:',tn)\n",
    "print('FN:',fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fpr = []\n",
    "w_all_fpr = []\n",
    "aug_y_true_sum = 0\n",
    "for counter in range(41):\n",
    "    aug_y_true = [1 if x == counter else 0 for x in all_labels]\n",
    "    aug_y_pred = [1 if x == counter else 0 for x in ap]\n",
    "    confusion = sklearn.metrics.confusion_matrix(y_true=aug_y_true, y_pred=aug_y_pred)\n",
    "    tn, fp, fn, tp = confusion.ravel()\n",
    "    all_fpr.append(fp/(fp+tn))  ## FPR\n",
    "    w_all_fpr.append((fp/(fp+tn))*aug_y_true.count(1))  ## w_FPR\n",
    "    aug_y_true_sum += aug_y_true.count(1)\n",
    "\n",
    "print('FPR: ', sum(all_fpr)/41.0*100.0)\n",
    "print('Weighted FPR: ', sum(w_all_fpr)/aug_y_true_sum*100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "confusion = sklearn.metrics.confusion_matrix(y_true=all_labels, y_pred=ap)\n",
    "disp = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=confusion,display_labels=np.arange(0,41))\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "disp.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3 = pd.read_pickle('d2a_dev_paper.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3['vbc']=ap.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3.to_pickle('d2a_dev_paper.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAPTUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import LayerIntegratedGradients, TokenReferenceBase, visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_reference = TokenReferenceBase(reference_token_idx=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lig = LayerIntegratedGradients(model, model.module.embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_with_softmax(x):\n",
    "    if mydataset=='mvd':\n",
    "        output = torch.nn.functional.softmax(model(x))\n",
    "        ind = output[0].argmax()\n",
    "        pred = output[0][ind]\n",
    "        return(pred.item() ,1 if ind > 0 else 0)\n",
    "    else:\n",
    "        output = torch.nn.functional.softmax(model(x))\n",
    "        ind = output[0].argmax()\n",
    "        pred = output[0][ind]\n",
    "        return(pred.item(), ind.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawcodes = my_tokenizer.decode(xxx.codes[0].tolist()).split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,item in enumerate(rawcodes):\n",
    "    rawcodes[index] = item.replace('Ä',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accumalate couple samples in this array for visualization purposes\n",
    "vis_data_records_ig = []\n",
    "\n",
    "def interpret_sentence(model, encoded_codes, label, rawcodes):\n",
    "    input_indices = encoded_codes\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    #input_indices = torch.tensor(indexed, device=device)\n",
    "    #input_indices = input_indices.unsqueeze(0)\n",
    "    \n",
    "    # input_indices dim: [sequence_length]\n",
    "    seq_length = 1024\n",
    "\n",
    "    # predict\n",
    "    pred, pred_ind  = forward_with_softmax(input_indices)\n",
    "\n",
    "    # generate reference indices for each sample\n",
    "    reference_indices = token_reference.generate_reference(seq_length, device=device).unsqueeze(0)\n",
    "\n",
    "    # compute attributions and approximation delta using layer integrated gradients\n",
    "    attributions_ig, delta = lig.attribute(input_indices, reference_indices, \\\n",
    "                                           n_steps=500, return_convergence_delta=True)\n",
    "\n",
    "    print('pred: ', pred_ind, '(', '%.2f'%pred, ')', ', delta: ', abs(delta))\n",
    "\n",
    "    add_attributions_to_visualizer(attributions_ig, rawcodes, pred, pred_ind, label, delta, vis_data_records_ig)\n",
    "    \n",
    "def add_attributions_to_visualizer(attributions, text, pred, pred_ind, label, delta, vis_data_records):\n",
    "    attributions = attributions.sum(dim=2).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    attributions = attributions.cpu().detach().numpy()\n",
    "\n",
    "    # storing couple samples in an array for visualization purposes\n",
    "    vis_data_records.append(visualization.VisualizationDataRecord(\n",
    "                            attributions,\n",
    "                            pred,\n",
    "                            pred_ind,\n",
    "                            label,\n",
    "                            \"vulns\",\n",
    "                            attributions.sum(),\n",
    "                            text,\n",
    "                            delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_sentence(model, xxx.codes, xxx.label.item(), rawcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.softmax(model(xxx.codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
